---
apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus-config
  namespace: monitoring

data:
  prometheus.rules: |-
    groups:
    - name: PrometheusEmbeddedExporter
      rules:
        - alert: PrometheusJobMissing
          expr: 'absent(up{job="prometheus"})'
          for: 0m
          labels:
            severity: warning
          annotations:
            summary: Prometheus job missing (instance {{ $labels.instance }})
            description: "A Prometheus job has disappeared\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

        - alert: PrometheusTargetMissing
          expr: 'up == 0'
          for: 0m
          labels:
            severity: critical
          annotations:
            summary: Prometheus target missing (instance {{ $labels.instance }})
            description: "A Prometheus target has disappeared. An exporter might be crashed.\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

        - alert: PrometheusAllTargetsMissing
          expr: 'sum by (job) (up) == 0'
          for: 0m
          labels:
            severity: critical
          annotations:
            summary: Prometheus all targets missing (instance {{ $labels.instance }})
            description: "A Prometheus job does not have living target anymore.\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

        - alert: PrometheusTargetMissingWithWarmupTime
          expr: 'sum by (instance, job) ((up == 0) * on (instance) group_right(job) (node_time_seconds - node_boot_time_seconds > 600))'
          for: 0m
          labels:
            severity: critical
          annotations:
            summary: Prometheus target missing with warmup time (instance {{ $labels.instance }})
            description: "Allow a job time to start up (10 minutes) before alerting that it's down.\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

        - alert: PrometheusConfigurationReloadFailure
          expr: 'prometheus_config_last_reload_successful != 1'
          for: 0m
          labels:
            severity: warning
          annotations:
            summary: Prometheus configuration reload failure (instance {{ $labels.instance }})
            description: "Prometheus configuration reload error\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

        - alert: PrometheusTooManyRestarts
          expr: 'changes(process_start_time_seconds{job=~"prometheus|pushgateway|alertmanager"}[15m]) > 2'
          for: 0m
          labels:
            severity: warning
          annotations:
            summary: Prometheus too many restarts (instance {{ $labels.instance }})
            description: "Prometheus has restarted more than twice in the last 15 minutes. It might be crashlooping.\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

        - alert: PrometheusAlertmanagerJobMissing
          expr: 'absent(up{job="alertmanager"})'
          for: 0m
          labels:
            severity: warning
          annotations:
            summary: Prometheus AlertManager job missing (instance {{ $labels.instance }})
            description: "A Prometheus AlertManager job has disappeared\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

        - alert: PrometheusAlertmanagerConfigurationReloadFailure
          expr: 'alertmanager_config_last_reload_successful != 1'
          for: 0m
          labels:
            severity: warning
          annotations:
            summary: Prometheus AlertManager configuration reload failure (instance {{ $labels.instance }})
            description: "AlertManager configuration reload error\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

        - alert: PrometheusAlertmanagerConfigNotSynced
          expr: 'count(count_values("config_hash", alertmanager_config_hash)) > 1'
          for: 0m
          labels:
            severity: warning
          annotations:
            summary: Prometheus AlertManager config not synced (instance {{ $labels.instance }})
            description: "Configurations of AlertManager cluster instances are out of sync\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

        - alert: PrometheusNotConnectedToAlertmanager
          expr: 'prometheus_notifications_alertmanagers_discovered < 1'
          for: 0m
          labels:
            severity: critical
          annotations:
            summary: Prometheus not connected to alertmanager (instance {{ $labels.instance }})
            description: "Prometheus cannot connect the alertmanager\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

        - alert: PrometheusRuleEvaluationFailures
          expr: 'increase(prometheus_rule_evaluation_failures_total[3m]) > 0'
          for: 0m
          labels:
            severity: critical
          annotations:
            summary: Prometheus rule evaluation failures (instance {{ $labels.instance }})
            description: "Prometheus encountered {{ $value }} rule evaluation failures, leading to potentially ignored alerts.\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

        - alert: PrometheusTemplateTextExpansionFailures
          expr: 'increase(prometheus_template_text_expansion_failures_total[3m]) > 0'
          for: 0m
          labels:
            severity: critical
          annotations:
            summary: Prometheus template text expansion failures (instance {{ $labels.instance }})
            description: "Prometheus encountered {{ $value }} template text expansion failures\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

        - alert: PrometheusRuleEvaluationSlow
          expr: 'prometheus_rule_group_last_duration_seconds > prometheus_rule_group_interval_seconds'
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: Prometheus rule evaluation slow (instance {{ $labels.instance }})
            description: "Prometheus rule evaluation took more time than the scheduled interval. It indicates a slower storage backend access or too complex query.\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

        - alert: PrometheusNotificationsBacklog
          expr: 'min_over_time(prometheus_notifications_queue_length[10m]) > 0'
          for: 0m
          labels:
            severity: warning
          annotations:
            summary: Prometheus notifications backlog (instance {{ $labels.instance }})
            description: "The Prometheus notification queue has not been empty for 10 minutes\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

        - alert: PrometheusAlertmanagerNotificationFailing
          expr: 'rate(alertmanager_notifications_failed_total[1m]) > 0'
          for: 0m
          labels:
            severity: critical
          annotations:
            summary: Prometheus AlertManager notification failing (instance {{ $labels.instance }})
            description: "Alertmanager is failing sending notifications\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

        - alert: PrometheusTargetEmpty
          expr: 'prometheus_sd_discovered_targets == 0'
          for: 0m
          labels:
            severity: critical
          annotations:
            summary: Prometheus target empty (instance {{ $labels.instance }})
            description: "Prometheus has no target in service discovery\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

        - alert: PrometheusTargetScrapingSlow
          expr: 'prometheus_target_interval_length_seconds{quantile="0.9"} / on (interval, instance, job) prometheus_target_interval_length_seconds{quantile="0.5"} > 1.05'
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: Prometheus target scraping slow (instance {{ $labels.instance }})
            description: "Prometheus is scraping exporters slowly since it exceeded the requested interval time. Your Prometheus server is under-provisioned.\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

        - alert: PrometheusLargeScrape
          expr: 'increase(prometheus_target_scrapes_exceeded_sample_limit_total[10m]) > 10'
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: Prometheus large scrape (instance {{ $labels.instance }})
            description: "Prometheus has many scrapes that exceed the sample limit\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

        - alert: PrometheusTsdbCheckpointCreationFailures
          expr: 'increase(prometheus_tsdb_checkpoint_creations_failed_total[1m]) > 0'
          for: 0m
          labels:
            severity: critical
          annotations:
            summary: Prometheus TSDB checkpoint creation failures (instance {{ $labels.instance }})
            description: "Prometheus encountered {{ $value }} checkpoint creation failures\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

        - alert: PrometheusTsdbCheckpointDeletionFailures
          expr: 'increase(prometheus_tsdb_checkpoint_deletions_failed_total[1m]) > 0'
          for: 0m
          labels:
            severity: critical
          annotations:
            summary: Prometheus TSDB checkpoint deletion failures (instance {{ $labels.instance }})
            description: "Prometheus encountered {{ $value }} checkpoint deletion failures\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

        - alert: PrometheusTsdbCompactionsFailed
          expr: 'increase(prometheus_tsdb_compactions_failed_total[1m]) > 0'
          for: 0m
          labels:
            severity: critical
          annotations:
            summary: Prometheus TSDB compactions failed (instance {{ $labels.instance }})
            description: "Prometheus encountered {{ $value }} TSDB compactions failures\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

        - alert: PrometheusTsdbHeadTruncationsFailed
          expr: 'increase(prometheus_tsdb_head_truncations_failed_total[1m]) > 0'
          for: 0m
          labels:
            severity: critical
          annotations:
            summary: Prometheus TSDB head truncations failed (instance {{ $labels.instance }})
            description: "Prometheus encountered {{ $value }} TSDB head truncation failures\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

        - alert: PrometheusTsdbReloadFailures
          expr: 'increase(prometheus_tsdb_reloads_failures_total[1m]) > 0'
          for: 0m
          labels:
            severity: critical
          annotations:
            summary: Prometheus TSDB reload failures (instance {{ $labels.instance }})
            description: "Prometheus encountered {{ $value }} TSDB reload failures\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

        - alert: PrometheusTsdbWalCorruptions
          expr: 'increase(prometheus_tsdb_wal_corruptions_total[1m]) > 0'
          for: 0m
          labels:
            severity: critical
          annotations:
            summary: Prometheus TSDB WAL corruptions (instance {{ $labels.instance }})
            description: "Prometheus encountered {{ $value }} TSDB WAL corruptions\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

        - alert: PrometheusTsdbWalTruncationsFailed
          expr: 'increase(prometheus_tsdb_wal_truncations_failed_total[1m]) > 0'
          for: 0m
          labels:
            severity: critical
          annotations:
            summary: Prometheus TSDB WAL truncations failed (instance {{ $labels.instance }})
            description: "Prometheus encountered {{ $value }} TSDB WAL truncation failures\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

        - alert: PrometheusTimeserieCardinality
          expr: 'label_replace(count by(__name__) ({__name__=~".+"}), "name", "$1", "__name__", "(.+)") > 15000'
          for: 0m
          labels:
            severity: warning
          annotations:
            summary: Prometheus timeserie cardinality (instance {{ $labels.instance }})
            description: "The \"{{ $labels.name }}\" timeserie cardinality is getting very high: {{ $value }}\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

    - name: BlackboxExporter
      rules:
        - alert: BlackboxProbeFailed
          expr: 'probe_success == 0'
          for: 5m
          labels:
            severity: critical
          annotations:
            summary: Blackbox probe failed (instance {{ $labels.instance }})
            description: "Probe failed\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

        - alert: BlackboxConfigurationReloadFailure
          expr: 'blackbox_exporter_config_last_reload_successful != 1'
          for: 0m
          labels:
            severity: warning
          annotations:
            summary: Blackbox configuration reload failure (instance {{ $labels.instance }})
            description: "Blackbox configuration reload failure\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

        - alert: BlackboxSlowProbe
          expr: 'avg_over_time(probe_duration_seconds[5m]) > 5'
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: Blackbox slow probe (instance {{ $labels.instance }})
            description: "Blackbox probe took more than 5s to complete\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

        - alert: BlackboxProbeHttpFailure
          expr: 'probe_http_status_code <= 199 OR probe_http_status_code >= 400'
          for: 5m
          labels:
            severity: critical
          annotations:
            summary: Blackbox probe HTTP failure (instance {{ $labels.instance }})
            description: "HTTP status code is not 200-399\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

        - alert: BlackboxSslCertificateWillExpireSoon
          expr: '3 <= round((last_over_time(probe_ssl_earliest_cert_expiry[10m]) - time()) / 86400, 0.1) < 20'
          for: 0m
          labels:
            severity: warning
          annotations:
            summary: Blackbox SSL certificate will expire soon (instance {{ $labels.instance }})
            description: "SSL certificate expires in less than 20 days\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

        - alert: BlackboxSslCertificateWillExpireSoon
          expr: '0 <= round((last_over_time(probe_ssl_earliest_cert_expiry[10m]) - time()) / 86400, 0.1) < 3'
          for: 0m
          labels:
            severity: critical
          annotations:
            summary: Blackbox SSL certificate will expire soon (instance {{ $labels.instance }})
            description: "SSL certificate expires in less than 3 days\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

        - alert: BlackboxSslCertificateExpired
          expr: 'round((last_over_time(probe_ssl_earliest_cert_expiry[10m]) - time()) / 86400, 0.1) < 0'
          for: 0m
          labels:
            severity: critical
          annotations:
            summary: Blackbox SSL certificate expired (instance {{ $labels.instance }})
            description: "SSL certificate has expired already\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

        - alert: BlackboxProbeSlowHttp
          expr: 'avg_over_time(probe_http_duration_seconds[5m]) > 2'
          for: 1m
          labels:
            severity: warning
          annotations:
            summary: Blackbox probe slow HTTP (instance {{ $labels.instance }})
            description: "HTTP request took more than 1s\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

        - alert: BlackboxProbeSlowPing
          expr: 'avg_over_time(probe_icmp_duration_seconds[1m]) > 1'
          for: 1m
          labels:
            severity: warning
          annotations:
            summary: Blackbox probe slow ping (instance {{ $labels.instance }})
            description: "Blackbox ping took more than 1s\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

    - name: TraefikEmbeddedExporterV2
      rules:
        - alert: TraefikServiceDown
          expr: 'count(traefik_service_server_up) by (service) == 0'
          for: 0m
          labels:
            severity: critical
          annotations:
            summary: Traefik service down (instance {{ $labels.instance }})
            description: "All Traefik services are down\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

        - alert: TraefikHighHttp5xxErrorRateService
          expr: 'sum(rate(traefik_service_requests_total{code=~"5.*"}[3m])) by (service) / sum(rate(traefik_service_requests_total[3m])) by (service) * 100 > 5'
          for: 1m
          labels:
            severity: critical
          annotations:
            summary: Traefik high HTTP 5xx error rate service (instance {{ $labels.instance }})
            description: "Traefik service 5xx error rate is above 5%\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

    - name: longhorn.rules
      rules:
      - alert: LonghornVolumeActualSpaceUsedWarning
        annotations:
          description: The actual space used by Longhorn volume {{$labels.volume}} on {{$labels.node}} is at {{$value}}% capacity for
            more than 5 minutes.
          summary: The actual used space of Longhorn volume is over 90% of the capacity.
        expr: (longhorn_volume_actual_size_bytes / longhorn_volume_capacity_bytes) * 100 > 90
        for: 5m
        labels:
          issue: The actual used space of Longhorn volume {{$labels.volume}} on {{$labels.node}} is high.
          severity: warning
      - alert: LonghornVolumeStatusCritical
        annotations:
          description: Longhorn volume {{$labels.volume}} on {{$labels.node}} is Fault for
            more than 2 minutes.
          summary: Longhorn volume {{$labels.volume}} is Fault
        expr: longhorn_volume_robustness == 3
        for: 5m
        labels:
          issue: Longhorn volume {{$labels.volume}} is Fault.
          severity: critical
      - alert: LonghornVolumeStatusWarning
        annotations:
          description: Longhorn volume {{$labels.volume}} on {{$labels.node}} is Degraded for
            more than 5 minutes.
          summary: Longhorn volume {{$labels.volume}} is Degraded
        expr: longhorn_volume_robustness == 2
        for: 5m
        labels:
          issue: Longhorn volume {{$labels.volume}} is Degraded.
          severity: warning
      - alert: LonghornNodeStorageWarning
        annotations:
          description: The used storage of node {{$labels.node}} is at {{$value}}% capacity for
            more than 5 minutes.
          summary:  The used storage of node is over 75% of the capacity.
        expr: (longhorn_node_storage_usage_bytes / longhorn_node_storage_capacity_bytes) * 100 > 75
        for: 5m
        labels:
          issue: The used storage of node {{$labels.node}} is high.
          severity: warning
      - alert: LonghornDiskStorageWarning
        annotations:
          description: The used storage of disk {{$labels.disk}} on node {{$labels.node}} is at {{$value}}% capacity for
            more than 5 minutes.
          summary:  The used storage of disk is over 75% of the capacity.
        expr: (longhorn_disk_usage_bytes / longhorn_disk_capacity_bytes) * 100 > 75
        for: 5m
        labels:
          issue: The used storage of disk {{$labels.disk}} on node {{$labels.node}} is high.
          severity: warning
      - alert: LonghornNodeDown
        annotations:
          description: There are {{$value}} Longhorn nodes which have been offline for more than 5 minutes.
          summary: Longhorn nodes is offline
        expr: (avg(longhorn_node_count_total) or on() vector(0)) - (count(longhorn_node_status{condition="ready"} == 1) or on() vector(0)) > 0
        for: 5m
        labels:
          issue: There are {{$value}} Longhorn nodes are offline
          severity: critical
      - alert: LonghornIntanceManagerCPUUsageWarning
        annotations:
          description: Longhorn instance manager {{$labels.instance_manager}} on {{$labels.node}} has CPU Usage / CPU request is {{$value}}% for
            more than 5 minutes.
          summary: Longhorn instance manager {{$labels.instance_manager}} on {{$labels.node}} has CPU Usage / CPU request is over 300%.
        expr: (longhorn_instance_manager_cpu_usage_millicpu/longhorn_instance_manager_cpu_requests_millicpu) * 100 > 300
        for: 5m
        labels:
          issue: Longhorn instance manager {{$labels.instance_manager}} on {{$labels.node}} consumes 3 times the CPU request.
          severity: warning
      - alert: LonghornNodeCPUUsageWarning
        annotations:
          description: Longhorn node {{$labels.node}} has CPU Usage / CPU capacity is {{$value}}% for
            more than 5 minutes.
          summary: Longhorn node {{$labels.node}} experiences high CPU pressure for more than 5m.
        expr: (longhorn_node_cpu_usage_millicpu / longhorn_node_cpu_capacity_millicpu) * 100 > 90
        for: 5m
        labels:
          issue: Longhorn node {{$labels.node}} experiences high CPU pressure.
          severity: warning

    - name: n8n
      rules:
      - alert: N8N failed workflow
        annotations:
          description: A n8n workflow has failed. Check https://n8n.n8n.svc.cluster.local/executions for more details
          summary: A n8n workflow has failed
        expr: changes(n8n_workflow_failed_total[10m]) > 1 
        for: 0m
        labels:
          issue: A n8n workflow has failed. Check https://n8n.n8n.svc.cluster.local/executions for more details
          severity: warning

  prometheus.yml: |
    global:
      scrape_interval: 10s
    rule_files:
      - /etc/prometheus/prometheus.rules
    alerting:
      alertmanagers:
      - scheme: http
        static_configs:
        - targets:
          - "alertmanager.monitoring.svc.cluster.local:9093"

    scrape_configs:
    - job_name: 'alertmanager'
      static_configs:
      - targets: ['alertmanager.monitoring.svc.cluster.local:9093']

    - job_name: 'blackbox_2xx'
      metrics_path: /probe
      params:
        module: [http_2xx]
      static_configs:
       - targets:
          - https://00-dev.galactae.eu
          - https://01-milkyway.galactae.eu
          - https://01.cdn.elanis.eu/portfolio/img/Elanis.png
          - https://alchemistry-leaderboard.dysnomia.studio/leaderboard/
          - https://blog.dysnomia.studio
          - https://cdn.galactae.eu/website/img/flags/fr.png
          - https://cloud.dysnomia.studio/media/favicons/favicon.png
          - https://dysnomia.studio
          - https://www.dysnomia.studio
          - https://elanis.eu
          - https://galactae.eu
          - https://gaspar.ovh
          - https://www.gaspar.ovh
          - https://leaderboard.dysnomia.studio/alchemistry/game/
          - https://leaderboard.dysnomia.studio/alchemistry/demo/
          - https://sonar.dysnomia.studio
          - https://wiki.dysnomia.studio
      relabel_configs:
      - source_labels: [__address__]
        target_label: __param_target
      - source_labels: [__param_target]
        target_label: instance
      - target_label: __address__
        replacement: blackbox-exporter.monitoring.svc.cluster.local:9115

    - job_name: 'blackbox_301'
      metrics_path: /probe
      params:
        module: [http_301]
      static_configs:
       - targets:
          - http://00-dev.galactae.eu
          - http://01-milkyway.galactae.eu
          - http://01.cdn.elanis.eu/portfolio/img/Elanis.png
          - http://www.achieve.games
          - http://achieve.games
          - https://achieve.games
          - https://www.achieve.games
          - http://alchemistry-leaderboard.dysnomia.studio/leaderboard/
          - http://blog.dysnomia.studio
          - http://cdn.galactae.eu
          - http://cloud.dysnomia.studio
          - http://dysnomia.studio
          - http://www.dysnomia.studio
          - http://elanis.eu
          - http://www.elanis.eu
          - https://www.elanis.eu
          - http://elanis.fr
          - https://elanis.fr
          - http://www.elanis.fr
          - https://www.elanis.fr
          - http://galactae.com
          - https://galactae.com
          - http://www.galactae.com
          - https://www.galactae.com
          - http://galactae.eu
          - http://www.galactae.eu
          - https://www.galactae.eu
          - http://galactae.space
          - https://galactae.space
          - http://www.galactae.space
          - https://www.galactae.space
          - http://gaspar.ovh
          - http://www.gaspar.ovh
          - http://leaderboard.dysnomia.studio/alchemistry/game/
          - http://prd.manufacturinc.dysnomia.studio
          - http://ptb.manufacturinc.dysnomia.studio
          - http://sonar.dysnomia.studio
          - http://wiki.dysnomia.studio
      relabel_configs:
      - source_labels: [__address__]
        target_label: __param_target
      - source_labels: [__param_target]
        target_label: instance
      - target_label: __address__
        replacement: blackbox-exporter.monitoring.svc.cluster.local:9115

    - job_name: 'longhorn'
      kubernetes_sd_configs:
        - role: endpoints
          namespaces:
            names: ['longhorn-system']
      relabel_configs:
      - source_labels: [__meta_kubernetes_endpoints_name]
        regex: 'longhorn-backend'
        action: keep
      - source_labels: [__meta_kubernetes_pod_container_port_number]
        action: drop
        regex: 9501
      - source_labels: [__meta_kubernetes_pod_container_port_number]
        action: drop
        regex: 9502
      - source_labels: [__meta_kubernetes_pod_container_port_number]
        action: drop
        regex: 9503

    - job_name: 'n8n'
      static_configs:
      - targets: ['n8n.n8n.svc.cluster.local']

    - job_name: 'opentelemetry-collector-deployment'
      static_configs:
      - targets: ['opentelemetry-deployment-collector-opentelemetry-collector.monitoring.svc.cluster.local:11235']

    - job_name: 'opentelemetry-collector-deamonset'
      scrape_interval: 15s
      kubernetes_sd_configs:
        - role: pod
          selectors:
            - role: "pod"
              label: "app.kubernetes.io/instance=opentelemetry-demonset-collector"
      relabel_configs:
        - source_labels: [__meta_kubernetes_pod_container_port_number]
          action: keep
          regex: 11234

    - job_name: 'prometheus'
      scrape_interval: 5s
      static_configs:
      - targets: ['localhost:9090']

    - job_name: 'traefik'
      static_configs:
      - targets: ['traefik-private.kube-system.svc.cluster.local:9100']
