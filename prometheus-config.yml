---
apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus-config
  namespace: kube-system

data:
  prometheus.rules: |-
    groups:
    - name: PrometheusEmbeddedExporter
      rules:
        - alert: PrometheusJobMissing
          expr: 'absent(up{job="prometheus"})'
          for: 0m
          labels:
            severity: warning
          annotations:
            summary: Prometheus job missing (instance {{ $labels.instance }})
            description: "A Prometheus job has disappeared\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

        - alert: PrometheusTargetMissing
          expr: 'up == 0'
          for: 0m
          labels:
            severity: critical
          annotations:
            summary: Prometheus target missing (instance {{ $labels.instance }})
            description: "A Prometheus target has disappeared. An exporter might be crashed.\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

        - alert: PrometheusAllTargetsMissing
          expr: 'sum by (job) (up) == 0'
          for: 0m
          labels:
            severity: critical
          annotations:
            summary: Prometheus all targets missing (instance {{ $labels.instance }})
            description: "A Prometheus job does not have living target anymore.\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

        - alert: PrometheusTargetMissingWithWarmupTime
          expr: 'sum by (instance, job) ((up == 0) * on (instance) group_right(job) (node_time_seconds - node_boot_time_seconds > 600))'
          for: 0m
          labels:
            severity: critical
          annotations:
            summary: Prometheus target missing with warmup time (instance {{ $labels.instance }})
            description: "Allow a job time to start up (10 minutes) before alerting that it's down.\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

        - alert: PrometheusConfigurationReloadFailure
          expr: 'prometheus_config_last_reload_successful != 1'
          for: 0m
          labels:
            severity: warning
          annotations:
            summary: Prometheus configuration reload failure (instance {{ $labels.instance }})
            description: "Prometheus configuration reload error\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

        - alert: PrometheusTooManyRestarts
          expr: 'changes(process_start_time_seconds{job=~"prometheus|pushgateway|alertmanager"}[15m]) > 2'
          for: 0m
          labels:
            severity: warning
          annotations:
            summary: Prometheus too many restarts (instance {{ $labels.instance }})
            description: "Prometheus has restarted more than twice in the last 15 minutes. It might be crashlooping.\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

        - alert: PrometheusAlertmanagerJobMissing
          expr: 'absent(up{job="alertmanager"})'
          for: 0m
          labels:
            severity: warning
          annotations:
            summary: Prometheus AlertManager job missing (instance {{ $labels.instance }})
            description: "A Prometheus AlertManager job has disappeared\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

        - alert: PrometheusAlertmanagerConfigurationReloadFailure
          expr: 'alertmanager_config_last_reload_successful != 1'
          for: 0m
          labels:
            severity: warning
          annotations:
            summary: Prometheus AlertManager configuration reload failure (instance {{ $labels.instance }})
            description: "AlertManager configuration reload error\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

        - alert: PrometheusAlertmanagerConfigNotSynced
          expr: 'count(count_values("config_hash", alertmanager_config_hash)) > 1'
          for: 0m
          labels:
            severity: warning
          annotations:
            summary: Prometheus AlertManager config not synced (instance {{ $labels.instance }})
            description: "Configurations of AlertManager cluster instances are out of sync\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

        - alert: PrometheusNotConnectedToAlertmanager
          expr: 'prometheus_notifications_alertmanagers_discovered < 1'
          for: 0m
          labels:
            severity: critical
          annotations:
            summary: Prometheus not connected to alertmanager (instance {{ $labels.instance }})
            description: "Prometheus cannot connect the alertmanager\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

        - alert: PrometheusRuleEvaluationFailures
          expr: 'increase(prometheus_rule_evaluation_failures_total[3m]) > 0'
          for: 0m
          labels:
            severity: critical
          annotations:
            summary: Prometheus rule evaluation failures (instance {{ $labels.instance }})
            description: "Prometheus encountered {{ $value }} rule evaluation failures, leading to potentially ignored alerts.\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

        - alert: PrometheusTemplateTextExpansionFailures
          expr: 'increase(prometheus_template_text_expansion_failures_total[3m]) > 0'
          for: 0m
          labels:
            severity: critical
          annotations:
            summary: Prometheus template text expansion failures (instance {{ $labels.instance }})
            description: "Prometheus encountered {{ $value }} template text expansion failures\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

        - alert: PrometheusRuleEvaluationSlow
          expr: 'prometheus_rule_group_last_duration_seconds > prometheus_rule_group_interval_seconds'
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: Prometheus rule evaluation slow (instance {{ $labels.instance }})
            description: "Prometheus rule evaluation took more time than the scheduled interval. It indicates a slower storage backend access or too complex query.\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

        - alert: PrometheusNotificationsBacklog
          expr: 'min_over_time(prometheus_notifications_queue_length[10m]) > 0'
          for: 0m
          labels:
            severity: warning
          annotations:
            summary: Prometheus notifications backlog (instance {{ $labels.instance }})
            description: "The Prometheus notification queue has not been empty for 10 minutes\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

        - alert: PrometheusAlertmanagerNotificationFailing
          expr: 'rate(alertmanager_notifications_failed_total[1m]) > 0'
          for: 0m
          labels:
            severity: critical
          annotations:
            summary: Prometheus AlertManager notification failing (instance {{ $labels.instance }})
            description: "Alertmanager is failing sending notifications\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

        - alert: PrometheusTargetEmpty
          expr: 'prometheus_sd_discovered_targets == 0'
          for: 0m
          labels:
            severity: critical
          annotations:
            summary: Prometheus target empty (instance {{ $labels.instance }})
            description: "Prometheus has no target in service discovery\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

        - alert: PrometheusTargetScrapingSlow
          expr: 'prometheus_target_interval_length_seconds{quantile="0.9"} / on (interval, instance, job) prometheus_target_interval_length_seconds{quantile="0.5"} > 1.05'
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: Prometheus target scraping slow (instance {{ $labels.instance }})
            description: "Prometheus is scraping exporters slowly since it exceeded the requested interval time. Your Prometheus server is under-provisioned.\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

        - alert: PrometheusLargeScrape
          expr: 'increase(prometheus_target_scrapes_exceeded_sample_limit_total[10m]) > 10'
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: Prometheus large scrape (instance {{ $labels.instance }})
            description: "Prometheus has many scrapes that exceed the sample limit\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

        - alert: PrometheusTargetScrapeDuplicate
          expr: 'increase(prometheus_target_scrapes_sample_duplicate_timestamp_total[5m]) > 0'
          for: 0m
          labels:
            severity: warning
          annotations:
            summary: Prometheus target scrape duplicate (instance {{ $labels.instance }})
            description: "Prometheus has many samples rejected due to duplicate timestamps but different values\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

        - alert: PrometheusTsdbCheckpointCreationFailures
          expr: 'increase(prometheus_tsdb_checkpoint_creations_failed_total[1m]) > 0'
          for: 0m
          labels:
            severity: critical
          annotations:
            summary: Prometheus TSDB checkpoint creation failures (instance {{ $labels.instance }})
            description: "Prometheus encountered {{ $value }} checkpoint creation failures\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

        - alert: PrometheusTsdbCheckpointDeletionFailures
          expr: 'increase(prometheus_tsdb_checkpoint_deletions_failed_total[1m]) > 0'
          for: 0m
          labels:
            severity: critical
          annotations:
            summary: Prometheus TSDB checkpoint deletion failures (instance {{ $labels.instance }})
            description: "Prometheus encountered {{ $value }} checkpoint deletion failures\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

        - alert: PrometheusTsdbCompactionsFailed
          expr: 'increase(prometheus_tsdb_compactions_failed_total[1m]) > 0'
          for: 0m
          labels:
            severity: critical
          annotations:
            summary: Prometheus TSDB compactions failed (instance {{ $labels.instance }})
            description: "Prometheus encountered {{ $value }} TSDB compactions failures\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

        - alert: PrometheusTsdbHeadTruncationsFailed
          expr: 'increase(prometheus_tsdb_head_truncations_failed_total[1m]) > 0'
          for: 0m
          labels:
            severity: critical
          annotations:
            summary: Prometheus TSDB head truncations failed (instance {{ $labels.instance }})
            description: "Prometheus encountered {{ $value }} TSDB head truncation failures\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

        - alert: PrometheusTsdbReloadFailures
          expr: 'increase(prometheus_tsdb_reloads_failures_total[1m]) > 0'
          for: 0m
          labels:
            severity: critical
          annotations:
            summary: Prometheus TSDB reload failures (instance {{ $labels.instance }})
            description: "Prometheus encountered {{ $value }} TSDB reload failures\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

        - alert: PrometheusTsdbWalCorruptions
          expr: 'increase(prometheus_tsdb_wal_corruptions_total[1m]) > 0'
          for: 0m
          labels:
            severity: critical
          annotations:
            summary: Prometheus TSDB WAL corruptions (instance {{ $labels.instance }})
            description: "Prometheus encountered {{ $value }} TSDB WAL corruptions\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

        - alert: PrometheusTsdbWalTruncationsFailed
          expr: 'increase(prometheus_tsdb_wal_truncations_failed_total[1m]) > 0'
          for: 0m
          labels:
            severity: critical
          annotations:
            summary: Prometheus TSDB WAL truncations failed (instance {{ $labels.instance }})
            description: "Prometheus encountered {{ $value }} TSDB WAL truncation failures\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

        - alert: PrometheusTimeserieCardinality
          expr: 'label_replace(count by(__name__) ({__name__=~".+"}), "name", "$1", "__name__", "(.+)") > 10000'
          for: 0m
          labels:
            severity: warning
          annotations:
            summary: Prometheus timeserie cardinality (instance {{ $labels.instance }})
            description: "The \"{{ $labels.name }}\" timeserie cardinality is getting very high: {{ $value }}\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

    - name: BlackboxExporter
      rules:
        - alert: BlackboxProbeFailed
          expr: 'probe_success == 0'
          for: 0m
          labels:
            severity: critical
          annotations:
            summary: Blackbox probe failed (instance {{ $labels.instance }})
            description: "Probe failed\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

        - alert: BlackboxConfigurationReloadFailure
          expr: 'blackbox_exporter_config_last_reload_successful != 1'
          for: 0m
          labels:
            severity: warning
          annotations:
            summary: Blackbox configuration reload failure (instance {{ $labels.instance }})
            description: "Blackbox configuration reload failure\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

        - alert: BlackboxSlowProbe
          expr: 'avg_over_time(probe_duration_seconds[1m]) > 1'
          for: 1m
          labels:
            severity: warning
          annotations:
            summary: Blackbox slow probe (instance {{ $labels.instance }})
            description: "Blackbox probe took more than 1s to complete\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

        - alert: BlackboxProbeHttpFailure
          expr: 'probe_http_status_code <= 199 OR probe_http_status_code >= 400'
          for: 0m
          labels:
            severity: critical
          annotations:
            summary: Blackbox probe HTTP failure (instance {{ $labels.instance }})
            description: "HTTP status code is not 200-399\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

        - alert: BlackboxSslCertificateWillExpireSoon
          expr: '3 <= round((last_over_time(probe_ssl_earliest_cert_expiry[10m]) - time()) / 86400, 0.1) < 20'
          for: 0m
          labels:
            severity: warning
          annotations:
            summary: Blackbox SSL certificate will expire soon (instance {{ $labels.instance }})
            description: "SSL certificate expires in less than 20 days\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

        - alert: BlackboxSslCertificateWillExpireSoon
          expr: '0 <= round((last_over_time(probe_ssl_earliest_cert_expiry[10m]) - time()) / 86400, 0.1) < 3'
          for: 0m
          labels:
            severity: critical
          annotations:
            summary: Blackbox SSL certificate will expire soon (instance {{ $labels.instance }})
            description: "SSL certificate expires in less than 3 days\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

        - alert: BlackboxSslCertificateExpired
          expr: 'round((last_over_time(probe_ssl_earliest_cert_expiry[10m]) - time()) / 86400, 0.1) < 0'
          for: 0m
          labels:
            severity: critical
          annotations:
            summary: Blackbox SSL certificate expired (instance {{ $labels.instance }})
            description: "SSL certificate has expired already\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

        - alert: BlackboxProbeSlowHttp
          expr: 'avg_over_time(probe_http_duration_seconds[1m]) > 1'
          for: 1m
          labels:
            severity: warning
          annotations:
            summary: Blackbox probe slow HTTP (instance {{ $labels.instance }})
            description: "HTTP request took more than 1s\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

        - alert: BlackboxProbeSlowPing
          expr: 'avg_over_time(probe_icmp_duration_seconds[1m]) > 1'
          for: 1m
          labels:
            severity: warning
          annotations:
            summary: Blackbox probe slow ping (instance {{ $labels.instance }})
            description: "Blackbox ping took more than 1s\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

    - name: TraefikEmbeddedExporterV2
      rules:
        - alert: TraefikServiceDown
          expr: 'count(traefik_service_server_up) by (service) == 0'
          for: 0m
          labels:
            severity: critical
          annotations:
            summary: Traefik service down (instance {{ $labels.instance }})
            description: "All Traefik services are down\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

        - alert: TraefikHighHttp4xxErrorRateService
          expr: 'sum(rate(traefik_service_requests_total{code=~"4.*"}[3m])) by (service) / sum(rate(traefik_service_requests_total[3m])) by (service) * 100 > 5'
          for: 1m
          labels:
            severity: critical
          annotations:
            summary: Traefik high HTTP 4xx error rate service (instance {{ $labels.instance }})
            description: "Traefik service 4xx error rate is above 5%\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

        - alert: TraefikHighHttp5xxErrorRateService
          expr: 'sum(rate(traefik_service_requests_total{code=~"5.*"}[3m])) by (service) / sum(rate(traefik_service_requests_total[3m])) by (service) * 100 > 5'
          for: 1m
          labels:
            severity: critical
          annotations:
            summary: Traefik high HTTP 5xx error rate service (instance {{ $labels.instance }})
            description: "Traefik service 5xx error rate is above 5%\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

  prometheus.yml: |
    global:
      scrape_interval: 10s
    rule_files:
      - /etc/prometheus/prometheus.rules
    alerting:
      alertmanagers:
      - scheme: http
        static_configs:
        - targets:
          - "alertmanager.monitoring.svc.cluster.local:9093"

    scrape_configs:
    - job_name: 'alertmanager'
      static_configs:
      - targets: ['alertmanager.monitoring.svc.cluster.local:9093']

    - job_name: 'blackbox_2xx'
      metrics_path: /probe
      params:
        module: [http_2xx]
      static_configs:
       - targets:
          - https://achieve.games
          - https://www.achieve.games
          - https://alchemistry-leaderboard.dysnomia.studio/leaderboard/
          - https://downstat.us
          - https://www.downstat.us
          - https://01.cdn.elanis.eu/portfolio/img/Elanis.png
          - https://elanis.eu
          - https://www.elanis.eu
          - https://elanis.fr
          - https://www.elanis.fr
          - https://www.gaspar.ovh
          - https://gaspar.ovh
          - https://sonar.dysnomia.studio
      relabel_configs:
      - source_labels: [__address__]
        target_label: __param_target
      - source_labels: [__param_target]
        target_label: instance
      - target_label: __address__
        replacement: blackbox-exporter.kube-system.svc.cluster.local:9115

    - job_name: 'blackbox_301'
      metrics_path: /probe
      params:
        module: [http_301]
      static_configs:
       - targets:
          - http://www.achieve.games
          - http://achieve.games
          - http://alchemistry-leaderboard.dysnomia.studio/leaderboard/
          - http://downstat.us
          - http://www.downstat.us
          - http://01.cdn.elanis.eu/portfolio/img/Elanis.png
          - http://www.elanis.eu
          - http://elanis.eu
          - http://www.elanis.fr
          - http://elanis.fr
          - http://www.gaspar.ovh
          - http://gaspar.ovh
          - http://prd.manufacturinc.dysnomia.studio
          - http://ptb.manufacturinc.dysnomia.studio
          - http://sonar.dysnomia.studio
      relabel_configs:
      - source_labels: [__address__]
        target_label: __param_target
      - source_labels: [__param_target]
        target_label: instance
      - target_label: __address__
        replacement: blackbox-exporter.kube-system.svc.cluster.local:9115

    - job_name: 'node-exporter'
      static_configs:
      - targets: ['node-exporter:9100']

    - job_name: 'pgwatch2'
      static_configs:
      - targets: ['pgwatch:9187']

    - job_name: 'prometheus'
      scrape_interval: 5s
      static_configs:
      - targets: ['localhost:9090']

    - job_name: 'traefik'
      static_configs:
      - targets: ['traefik-prometheus:9100']
